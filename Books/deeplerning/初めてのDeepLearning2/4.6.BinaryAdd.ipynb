{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "import numpy as np\r\n",
                "#import cupy as np\r\n",
                "import matplotlib.pyplot as plt\r\n",
                "\r\n",
                "%matplotlib inline"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "n_time = 8   # 時系列の数(今回は最大ビット数)\r\n",
                "n_in   = 2   # 入力層ニューロン数(二つの値を足し合わせる目的なので)\r\n",
                "n_mid  = 32  # 中間層ニューロン数(適当)\r\n",
                "n_out  = 1   # 出力層ニューロン数(真実はいつも一つ！)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "# 学習データ\r\n",
                "max_num = 2**n_time\r\n",
                "binaries = np.zeros((max_num, n_time), dtype=int)\r\n",
                "for i in range(max_num):\r\n",
                "    num10 = i\r\n",
                "    for j in range(n_time):\r\n",
                "        pow2 = 2 ** (n_time - 1 - j)\r\n",
                "        binaries[i, j] = num10 // pow2\r\n",
                "        num10 %= pow2\r\n",
                "\r\n",
                "print(binaries)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[[0 0 0 ... 0 0 0]\n",
                        " [0 0 0 ... 0 0 1]\n",
                        " [0 0 0 ... 0 1 0]\n",
                        " ...\n",
                        " [1 1 1 ... 1 0 1]\n",
                        " [1 1 1 ... 1 1 0]\n",
                        " [1 1 1 ... 1 1 1]]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "source": [
                "eta      = 0.1  # 学習係数\r\n",
                "n_learn = 5001  # 学習回数\r\n",
                "interval = 500  # 経過の表示間隔\r\n",
                "\r\n",
                "class OutputLayer:\r\n",
                "    def __init__(self, n_upper, n):\r\n",
                "        self.w = np.random.randn(n_upper, n) / np.sqrt(n_upper)\r\n",
                "        self.b = np.zeros(n)\r\n",
                "    \r\n",
                "    def activate_func(self, u):\r\n",
                "        # sigmoid function\r\n",
                "        return 1 / (1 + np.exp(-u))\r\n",
                "    \r\n",
                "    def diff_func(self, grad_y, y):\r\n",
                "        # differencial sigmoid\r\n",
                "        return grad_y * (1 - y) * y\r\n",
                "    \r\n",
                "    def forward(self, x):\r\n",
                "        self.x = x\r\n",
                "        u = np.dot(x, self.w) + self.b\r\n",
                "        self.y = self.activate_func(u)\r\n",
                "        return self.y\r\n",
                "    \r\n",
                "    def backward(self, x, y, t):\r\n",
                "        delta = self.diff_func(y - t, y)\r\n",
                "        self.grad_w = np.dot(x.T, delta)\r\n",
                "        self.grad_b = np.sum(delta, axis=0)\r\n",
                "        self.grad_x = np.dot(delta, self.w.T)\r\n",
                "        return self.grad_x\r\n",
                "\r\n",
                "    def reset_sum_grad(self):\r\n",
                "        self.grad_w = np.zeros_like(self.w)\r\n",
                "        self.grad_b = np.zeros_like(self.b)\r\n",
                "    \r\n",
                "    def update(self, eta):\r\n",
                "        self.w -= eta * self.grad_w\r\n",
                "        self.b -= eta * self.grad_b\r\n",
                "\r\n",
                "class RnnBaseLayer:\r\n",
                "    def __init__(self, n_upper, n):\r\n",
                "        self.w = np.random.randn(n_upper, n) / np.sqrt(n_upper)\r\n",
                "        self.v = np.random.randn(n, n) / np.sqrt(n)\r\n",
                "        self.b = np.zeros(n)\r\n",
                "    \r\n",
                "    def forward(self, x, prev_y):\r\n",
                "        u = np.dot(x, self.w) + np.dot(prev_y, self.v) + self.b\r\n",
                "        self.y = np.tanh(u)\r\n",
                "        return self.y\r\n",
                "    \r\n",
                "    def backward(self, x, y, prev_y, grad_y):\r\n",
                "        delta = grad_y * (1 - y**2)\r\n",
                "\r\n",
                "        self.grad_w += np.dot(x.T, delta)\r\n",
                "        self.grad_v += np.dot(prev_y.T, delta)\r\n",
                "        self.grad_b += np.sum(delta, axis=0)\r\n",
                "\r\n",
                "        self.grad_x = np.dot(delta, self.w.T)\r\n",
                "        self.grad_prev_y = np.dot(delta, self.v.T)\r\n",
                "        return self.grad_prev_y\r\n",
                "\r\n",
                "    def reset_sum_grad(self):\r\n",
                "        self.grad_w = np.zeros_like(self.w)\r\n",
                "        self.grad_v = np.zeros_like(self.v)\r\n",
                "        self.grad_b = np.zeros_like(self.b)\r\n",
                "\r\n",
                "    def update(self, eta):\r\n",
                "        self.w -= eta * self.grad_w\r\n",
                "        self.v -= eta * self.grad_v\r\n",
                "        self.b -= eta * self.grad_b"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "rnnLayer = RnnBaseLayer(n_in, n_mid)\r\n",
                "outputLayer = OutputLayer(n_mid, n_out)\r\n",
                "\r\n",
                "def train(x_mb, t_mb):\r\n",
                "    y_rnn = np.zeros((len(x_mb), n_time+1, n_mid))\r\n",
                "    y_out = np.zeros((len(x_mb), n_time, n_out))\r\n",
                "\r\n",
                "    # Forward propergation\r\n",
                "    y_prev = y_rnn[:, 0, :]\r\n",
                "    for i in range(n_time):\r\n",
                "        # RNN layer\r\n",
                "        x = x_mb[:, i, :]\r\n",
                "        y = rnnLayer.forward(x, y_prev)\r\n",
                "        y_rnn[:, i + 1, :] = y\r\n",
                "        y_prev = y\r\n",
                "\r\n",
                "        # output layer\r\n",
                "        y_out[:, i, :] = outputLayer.forward(y)\r\n",
                "    \r\n",
                "    # back propergation\r\n",
                "    outputLayer.reset_sum_grad()\r\n",
                "    rnnLayer.reset_sum_grad()\r\n",
                "    grad_y = 0\r\n",
                "    for i in reversed(range(n_time)):\r\n",
                "        # output layer\r\n",
                "        x = y_rnn[:, i+1, :]\r\n",
                "        y = y_out[:, i, :]\r\n",
                "        t = t_mb[:, i, :]\r\n",
                "        grad_x_out = outputLayer.backward(x, y, t)\r\n",
                "\r\n",
                "        # Rnn layer\r\n",
                "        x = x_mb[:, i, :]\r\n",
                "        y = y_rnn[:, i+1, :]\r\n",
                "        y_prev = y_rnn[:, i, :]\r\n",
                "        grad_y = rnnLayer.backward(x, y, y_prev, grad_y + grad_x_out)\r\n",
                "\r\n",
                "    # update\r\n",
                "    rnnLayer.update(eta)\r\n",
                "    outputLayer.update(eta)\r\n",
                "    return y_out\r\n",
                "\r\n",
                "def get_error(y, t):\r\n",
                "    return 1.0/2.0*np.sum(np.square(y - t))\r\n",
                "\r\n",
                "for i in range(n_learn):\r\n",
                "    # ランダムなインデックスを作成\r\n",
                "    num1 = np.random.randint(max_num//2)\r\n",
                "    num2 = np.random.randint(max_num//2)\r\n",
                "\r\n",
                "    # これをビット配列に置き換えて\r\n",
                "    x1= binaries[num1]\r\n",
                "    x2= binaries[num2]\r\n",
                "\r\n",
                "    # 引数の形状にまとめる\r\n",
                "    x_in = np.zeros((1, n_time, n_in))\r\n",
                "    x_in[0, :, 0] = x1\r\n",
                "    x_in[0, :, 1] = x2\r\n",
                "    x_in  = np.flip(x_in, axis=1)\r\n",
                "\r\n",
                "    # 結果データ\r\n",
                "    t = binaries[num1+num2]\r\n",
                "    t_in = t.reshape(1, n_time, n_out)\r\n",
                "    t_in = np.flip(t_in , axis=1)\r\n",
                "\r\n",
                "    # 学習\r\n",
                "    y_out = train(x_in, t_in)\r\n",
                "    y = np.flip(y_out, axis=1).reshape(-1)\r\n",
                "\r\n",
                "    error = get_error(y_out, t_in)\r\n",
                "\r\n",
                "    if i % interval == 0:\r\n",
                "        y2 = np.where(y<0.5, 0, 1)\r\n",
                "        y10 = 0\r\n",
                "        for j in range(len(y)):\r\n",
                "            pow2 = 2 ** (n_time-1-j)\r\n",
                "            y10 += y2[j] * pow2\r\n",
                "\r\n",
                "        print(\"learn count:\", i)\r\n",
                "        print(\"error rate:\", error)\r\n",
                "        c = \"Success : \" if (y2 == t).all() else \"Failure : \"\r\n",
                "        print(c + str(num1) + \" + \" + str(num2) + \" = \" + str(y10))\r\n",
                "        print(\"========================\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "learn count: 0\n",
                        "error rate: 0.8945524193072958\n",
                        "Failure : 42 + 106 = 129\n",
                        "========================\n",
                        "learn count: 500\n",
                        "error rate: 0.7436649947915788\n",
                        "Failure : 9 + 94 = 97\n",
                        "========================\n",
                        "learn count: 1000\n",
                        "error rate: 0.07706468189737165\n",
                        "Success : 88 + 100 = 188\n",
                        "========================\n",
                        "learn count: 1500\n",
                        "error rate: 0.01835847402480273\n",
                        "Success : 80 + 5 = 85\n",
                        "========================\n",
                        "learn count: 2000\n",
                        "error rate: 0.03029276681234873\n",
                        "Success : 60 + 26 = 86\n",
                        "========================\n",
                        "learn count: 2500\n",
                        "error rate: 0.014938136294659793\n",
                        "Success : 108 + 72 = 180\n",
                        "========================\n",
                        "learn count: 3000\n",
                        "error rate: 0.03752934463364099\n",
                        "Success : 79 + 127 = 206\n",
                        "========================\n",
                        "learn count: 3500\n",
                        "error rate: 0.01200082109662094\n",
                        "Success : 42 + 113 = 155\n",
                        "========================\n",
                        "learn count: 4000\n",
                        "error rate: 0.01512034921879103\n",
                        "Success : 10 + 124 = 134\n",
                        "========================\n",
                        "learn count: 4500\n",
                        "error rate: 0.009017929456911288\n",
                        "Success : 89 + 85 = 174\n",
                        "========================\n",
                        "learn count: 5000\n",
                        "error rate: 0.0023367408689698825\n",
                        "Success : 88 + 1 = 89\n",
                        "========================\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.8 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "73e03da126b73bfff3642ec5261d56fa25c444ea595de51041687efaa60dda41"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}