{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "73e03da126b73bfff3642ec5261d56fa25c444ea595de51041687efaa60dda41"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## エポックとバッチ\n",
    "\n",
    "1エポック = 全データを 1 回学習すること。  \n",
    "教師データのセット単位を 1 バッチ。\n",
    "\n",
    "1 エポック = 複数バッチ\n",
    "\n",
    "### バッチ学習\n",
    "\n",
    "バッチサイズ = エポックサイズ の学習のこと。  \n",
    "学習が安定してて高速ではあるが、局所解にハマりやすい。\n",
    "\n",
    "### オンライン学習\n",
    "\n",
    "バッチサイズが 1 となる。  \n",
    "個々のデータに振り回されるので、バーストには弱いが、局所解にとらわれにくい。\n",
    "\n",
    "### ミニバッチ学習\n",
    "\n",
    "訓練データを複数のバッチに分割して、バッチ単位で重みとバイアスの学習を行う。  \n",
    "局所解に囚われにくいし、個々のデータで振れ幅もさほどではない。\n",
    "\n",
    "\n",
    "1000 の教師データがあるとき、\n",
    "\n",
    "* バッチ学習「1000で1回重みとバイアス計算を行う」\n",
    "* オンライン学習「1エポック当たり 1000 回学習するよ」\n",
    "* ミニバッチ学習「1バッチ 50で設定したら、20 回学習するよ」\n",
    "\n",
    "## 行列での演算\n",
    "\n",
    "バッチサイズを 8 入力数(入力層のニューロン数) 3 とすると、入力を表す行列サイズは 8x3。  \n",
    "バッチサイズが 1 の時、1x3 になるので、ベクトルの様な形状になる。\n",
    "\n",
    "試しに 4x3 (バッチサイズ 2)の思考実験をすると"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [4, 5, 6, 7]\n",
    "])\n",
    "\n",
    "W = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "X.dot(W)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 70,  80,  90],\n",
       "       [136, 158, 180]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "source": [
    "もう少し汎用的に考えると バッチサイズ $h$ 、入力層 $m$ 、ニューロン数 $n$ の行列はこんな感じか\n",
    "\n",
    "$$\n",
    "XW = \\left(\n",
    "        \\begin{array}{xxxx}\n",
    "            x_{11} & x_{12} & ... & x_{1m} \\\\\n",
    "            x_{21} & x_{22} & ... & x_{2m} \\\\\n",
    "            ...    & ...    & ... & ... \\\\\n",
    "            x_{h1} & x_{h2} & ... & x_{hm}\n",
    "        \\end{array}\n",
    "    \\right)\n",
    "    \\left(\n",
    "        \\begin{array}{xxxx}\n",
    "            w_{11} & w_{12} & ... & w_{1n} \\\\\n",
    "            w_{21} & w_{22} & ... & w_{2n} \\\\\n",
    "            ...    & ...    & ... & ... \\\\\n",
    "            w_{m1} & w_{m2} & ... & w_{mn}\n",
    "        \\end{array}\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "うげ…でも数式書いてみると理解できる。  \n",
    "この時、座標 `1,1` の結果は\n",
    "\n",
    "$$\n",
    "x_{11}w_{11} + x_{12}w_{21} + ... + x_{1m}w_{m1}  \\\\\n",
    "= \\sum^m_{k=1} x_{1k}w_{k1}\n",
    "$$\n",
    "\n",
    "となるので、同様に考えて\n",
    "\n",
    "$$\n",
    "XY = \\left(\n",
    "        \\begin{array}{xxxx}\n",
    "            \\sum^m_{k=1} x_{1k}w_{k1} & \\sum^m_{k=1} x_{1k}w_{k2} & ... & \\sum^m_{k=1} x_{1k}w_{kn} \\\\\n",
    "            \\sum^m_{k=1} x_{2k}w_{k1} & \\sum^m_{k=1} x_{2k}w_{k2} & ... & \\sum^m_{k=1} x_{2k}w_{kn} \\\\\n",
    "            ...    & ...    & ... & ... \\\\\n",
    "            \\sum^m_{k=1} x_{hk}w_{k1} & \\sum^m_{k=1} x_{hk}w_{k2} & ... & \\sum^m_{k=1} x_{hk}w_{kn}\n",
    "        \\end{array}\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "で、バイアスはニューロン毎に定義されるので数は ニューロン数と同数の $n$\n",
    "\n",
    "$$\n",
    "XY+B = \\left(\n",
    "        \\begin{array}{xxxx}\n",
    "            \\sum^m_{k=1} x_{1k}w_{k1} + b_1 & \\sum^m_{k=1} x_{1k}w_{k2} + b_2 & ... & \\sum^m_{k=1} x_{1k}w_{kn} + b_n \\\\\n",
    "            \\sum^m_{k=1} x_{2k}w_{k1} + b_1 & \\sum^m_{k=1} x_{2k}w_{k2} + b_2 & ... & \\sum^m_{k=1} x_{2k}w_{kn} + b_n \\\\\n",
    "            ...    & ...    & ... & ... \\\\\n",
    "            \\sum^m_{k=1} x_{hk}w_{k1} + b_1 & \\sum^m_{k=1} x_{hk}w_{k2} + b_2 & ... & \\sum^m_{k=1} x_{hk}w_{kn} + b_n\n",
    "        \\end{array}\n",
    "    \\right) = U\n",
    "$$\n",
    "\n",
    "ここに活性関数 $f(x)$ を適用すると結果 Y は $ Y = f(U) $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 71  82  93]\n [137 160 183]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "## 値は適当だけど、実装的にはこな感じ\n",
    "\n",
    "X = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [4, 5, 6, 7]\n",
    "])\n",
    "\n",
    "W = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "B = np.array([ 1, 2, 3 ])\n",
    "\n",
    "U = X.dot(W) + B\n",
    "\n",
    "print(U)\n",
    "\n",
    "def sigmoid(u):\n",
    "    return 1 / (1 + np.exp(-u))\n",
    "\n",
    "sigmoid(U)"
   ]
  },
  {
   "source": [
    "さて、ネタは揃った、いや、揃っちゃった（汗  \n",
    "後はこの値に最終層(恒等関数 or ソフトマックス関数)を含んだ状態で微分して、勾配降下法で W, B の値を更新すればニューラルネットワークが完成するという。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}