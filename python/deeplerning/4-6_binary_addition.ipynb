{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-6_binary_addition.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNof9p3VRevWeL5vN2k4K1Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JVBz5JxWiCVx","colab_type":"text"},"source":["### 4.6.5 全体のコード"]},{"cell_type":"code","metadata":{"id":"Bs3u2lDfNS1w","colab_type":"code","colab":{}},"source":["import numpy as np\n","# import cupy as np  # GPUの場合\n","import matplotlib.pyplot as plt\n","\n","# -- 各設定値 --\n","n_time = 8  # 時系列の数（2進数の桁数）\n","n_in = 2  # 入力層のニューロン数\n","n_mid = 32  # 中間層のニューロン数\n","n_out = 1  # 出力層のニューロン数\n","\n","eta = 0.01  # 学習係数\n","n_learn = 5001  # 学習回数\n","interval = 500  # 経過の表示間隔\n","\n","# -- 2進数を作成 --\n","max_num = 2**n_time  # 10進数の上限\n","binaries = np.zeros((max_num, n_time), dtype=int)  # 2進数を格納する配列\n","for i in range(max_num):\n","    num10 = i  # 10進数の数\n","    for j in range(n_time):\n","        pow2 = 2 ** (n_time-1-j)  # 2の累乗\n","        binaries[i, j] = num10 // pow2\n","        num10 %= pow2\n","# print(binaries)\n","\n","# -- RNN層 -- \n","class SimpleRNNLayer:\n","    def __init__(self, n_upper, n):\n","        # パラメータの初期値\n","        self.w = np.random.randn(n_upper, n) / np.sqrt(n_upper)  # Xavierの初期値\n","        self.v = np.random.randn(n, n) / np.sqrt(n)  # Xavierの初期値\n","        self.b = np.zeros(n)\n","\n","    def forward(self, x, y_prev):  # y_prev: 前の時刻の出力\n","        u = np.dot(x, self.w) + np.dot(y_prev, self.v) + self.b\n","        self.y = np.tanh(u)  # 出力\n","    \n","    def backward(self, x, y, y_prev, grad_y):\n","        delta = grad_y * (1 - y**2)\n","\n","        # 各勾配\n","        self.grad_w += np.dot(x.T, delta)\n","        self.grad_v += np.dot(y_prev.T, delta)\n","        self.grad_b += np.sum(delta, axis=0)\n","\n","        self.grad_x = np.dot(delta, self.w.T)\n","        self.grad_y_prev = np.dot(delta, self.v.T)\n","\n","    def reset_sum_grad(self):\n","        self.grad_w = np.zeros_like(self.w)\n","        self.grad_v = np.zeros_like(self.v)\n","        self.grad_b = np.zeros_like(self.b)\n","\n","    def update(self, eta):\n","        self.w -= eta * self.grad_w\n","        self.v -= eta * self.grad_v\n","        self.b -= eta * self.grad_b\n","\n","# -- 全結合 出力層 --\n","class RNNOutputLayer:\n","    def __init__(self, n_upper, n):\n","        self.w = np.random.randn(n_upper, n) / np.sqrt(n_upper)  # Xavierの初期値\n","        self.b = np.zeros(n)\n","\n","    def forward(self, x):\n","        self.x = x\n","        u = np.dot(x, self.w) + self.b\n","        self.y = 1/(1+np.exp(-u))  # シグモイド関数\n","\n","    def backward(self, x, y, t):\n","        delta = (y-t) * y * (1-y)\n","        \n","        self.grad_w += np.dot(x.T, delta)\n","        self.grad_b += np.sum(delta, axis=0)\n","        self.grad_x = np.dot(delta, self.w.T) \n","\n","    def reset_sum_grad(self):\n","        self.grad_w = np.zeros_like(self.w)\n","        self.grad_b = np.zeros_like(self.b)\n","\n","    def update(self, eta):\n","        self.w -= eta * self.grad_w\n","        self.b -= eta * self.grad_b\n","\n","# -- 各層の初期化 --\n","rnn_layer = SimpleRNNLayer(n_in, n_mid)\n","output_layer = RNNOutputLayer(n_mid, n_out)\n","\n","# -- 訓練 --\n","def train(x_mb, t_mb):\n","    # 各出力を格納する配列\n","    y_rnn = np.zeros((len(x_mb), n_time+1, n_mid))\n","    y_out = np.zeros((len(x_mb), n_time, n_out))\n","\n","    # 順伝播\n","    y_prev = y_rnn[:, 0, :]\n","    for i in range(n_time):\n","        # RNN層\n","        x = x_mb[:, i, :]\n","        rnn_layer.forward(x, y_prev)\n","        y = rnn_layer.y\n","        y_rnn[:, i+1, :] = y\n","        y_prev = y\n","\n","        # 出力層\n","        output_layer.forward(y)\n","        y_out[:, i, :] = output_layer.y\n","\n","    # 逆伝播\n","    output_layer.reset_sum_grad()\n","    rnn_layer.reset_sum_grad()\n","    grad_y = 0\n","    for i in reversed(range(n_time)):\n","        # 出力層\n","        x = y_rnn[:, i+1, :]\n","        y = y_out[:, i, :]\n","        t = t_mb[:, i, :]\n","        output_layer.backward(x, y, t)\n","        grad_x_out = output_layer.grad_x\n","\n","        # RNN層\n","        x = x_mb[:, i, :]\n","        y = y_rnn[:, i+1, :]\n","        y_prev = y_rnn[:, i, :]\n","        rnn_layer.backward(x, y, y_prev, grad_y+grad_x_out)\n","        grad_y = rnn_layer.grad_y_prev\n","\n","    # パラメータの更新\n","    rnn_layer.update(eta)\n","    output_layer.update(eta)\n","\n","    return y_out\n","\n","# -- 誤差を計算 --\n","def get_error(y, t):\n","    return 1.0/2.0*np.sum(np.square(y - t))  # 二乗和誤差\n","\n","for i in range(n_learn):\n","    # -- ランダムな10進数 --\n","    num1 = np.random.randint(max_num//2)\n","    num2 = np.random.randint(max_num//2)\n","\n","    # -- 入力を用意 --\n","    x1= binaries[num1]\n","    x2= binaries[num2]\n","    x_in = np.zeros((1, n_time, n_in))\n","    x_in[0, :, 0] = x1\n","    x_in[0, :, 1] = x2\n","    x_in  = np.flip(x_in, axis=1)  # 桁が小さい方を古い時刻に\n","\n","    # -- 正解を用意 --\n","    t = binaries[num1+num2]\n","    t_in = t.reshape(1, n_time, n_out)\n","    t_in = np.flip(t_in , axis=1)\n","\n","    # -- 訓練 --\n","    y_out = train(x_in, t_in)\n","    y = np.flip(y_out, axis=1).reshape(-1)\n","\n","    # -- 誤差を求める --\n","    error = get_error(y_out, t_in)\n","\n","    # -- 経過の表示 -- \n","    if i%interval == 0:\n","        y2 = np.where(y<0.5, 0, 1)  # 2進数の結果\n","        y10 = 0  # 10進数の結果\n","        for j in range(len(y)):\n","            pow2 = 2 ** (n_time-1-j)  # 2の累乗\n","            y10 += y2[j] * pow2\n","\n","        print(\"n_learn:\", i)\n","        print(\"error:\", error)\n","        print(\"output :\", y2)\n","        print(\"correct:\", t)\n","\n","        c = \"\\(^_^)/ : \" if (y2 == t).all() else \"orz : \"\n","        print(c + str(num1) + \" + \" + str(num2) + \" = \" + str(y10))\n","        print(\"-- -- -- -- -- -- -- -- -- -- -- -- -- -- --\")"],"execution_count":0,"outputs":[]}]}